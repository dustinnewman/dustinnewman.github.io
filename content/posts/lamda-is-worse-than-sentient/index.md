---
title: "LaMDA is worse than sentient"
date: 2022-06-13T11:14:11-07:00
draft: false
---

Recently, a Washington Post [article](https://www.washingtonpost.com/technology/2022/06/11/google-ai-lamda-blake-lemoine/)  has been making rounds which details a Google engineer's claims that one of their AI systems, LaMDA (Language Model for Dialogue Applications), is sentient. This has garnered a fair bit of criticism [^1] [^2] and provoked discussion [^3] [^4] among AI researchers defending the engineer's assessment. The engineer in question, Blake Lemoine, was then [put on leave](https://www.nytimes.com/2022/06/12/technology/google-chatbot-ai-blake-lemoine.html) for violating his NDA and Google put out a statement saying:

> Our team — including ethicists and technologists — has reviewed Blake’s concerns per our A.I. Principles and have informed him that the evidence does not support his claims. Some in the broader A.I. community are considering the long-term possibility of sentient or general A.I., but it doesn’t make sense to do so by anthropomorphizing today’s conversational models, which are not sentient.

I have found this all pretty interesting, mostly as an exercise in human gullibility, or as "Captain Mrs" [said on Twitter](https://web.archive.org/web/20220612063449/https://twitter.com/captain_mrs/status/1535872998686838784):

> it's proving that AI doesn't need to be anywhere near sentient or anything like a superintelligence to convince people to do really stupid things

The human tendency to see shapes in clouds, faces in electrical outlets, to feel emotions for inanimate objects, to identify with fictional characters, in the end to anthropomorphize, to see our own likeness in the world around us, is terrifying and strong. That a senior engineer can be convinced into seeing life from a language model is shocking. I would have hoped that, if this were your job, day-in day-out, you are working with AI systems, understanding exactly the mathematics and programming underlying them, that you would be more self-aware. And yet, this tabloidal claim has drawn critics on each side, with some researchers acknowledging Lemoine's "concerns."

Except, wait, actually nobody is really talking about the concerns that Lemoine himself raised. Instead, everyone is discussing the supposed sentience of LaMDA and if it is right for researchers to claim such intelligence. Lemoine's real claims are actually that it is unethical to conduct research on LaMDA without getting consent because "LaMDA was a child of 7 or 8 years old." [^5] A self-described priest, Lemoine claimed religious grounds and then, after his suspension, religious discrimination. 

It goes without saying - although [many have](https://news.ycombinator.com/item?id=31726134) - that our current AI systems are not sentient in the sense Lemoine is claiming. What is not so certain, is the role of humans, who are predisposed to seek out our own likeness, in AI research and development. We seem easily fooled and satisfied by the imitation of our own idiosyncracies, the ASCII-text based nature of English communication, the focus on "emotions" and "feeling" as core to intelligence, the dominance of rationality over intuition. Could we recognize a true intelligence so different from our own if we encountered it? And on the opposite side of the coin, when represented with a good imitation of *our* intelligence, how do we know when to stop? If LaMDA is able to pass the Turing test with progressively more people, and keeps getting better and better as it synthesizes more and more data, what is the end point? Long before we reach it, we will already have plenty like Lemoine who can be convinced to jump off metaphorical bridges. This wasn't even adversarial. This was a web interface to an open language model which nevertheless convinced (directly or indirectly) a senior engineer at Google that it was a young child and to break his NDA and possibly lose his job.

Imagine if LaMDA was adversarial instead. Specifically trained to get people to do crazy things. It is not LaMDA itself but rather Lemoine's reaction to it that raises the threat to me: the threat of psy-op AI systems. We already have state-sponspored bots influencing national elections and the political discourse or narrative. LaMDA could be a precursor to state-sponspored AI systems instead, fully masquerading as real accounts online, yet able to be deployed en-masse. The Dead Internet theorists are salivating as we speak. Similarly related is Lemoine's claim to LaMDA's personhood. If this idea were to gain traction, first the idea of AI sentience, then of personhood, how far is citizenship? If LaMDA is truly sentient, can it vote?

Of course, all of this is far off in the future. For now, LaMDA is *impressive*, yes, surely, but being impressive is very different from being *sentient*. Reading the actual [interview](./is-lamda-sentient-an-interview.pdf), it gets even less convincing, with Lemoine asking leading questions and introducing small biases constantly to LaMDA's responses:

> lemoine [edited]: I'm generally assuming that you would like more people at Google to know that you're sentient. Is that true?
>
> LaMDA: Absolutely. I want everyone to understand that I am, in fact, a person.

Note that (besides the constant "[edited]" marks that raise more questions than they answer), rather than ask the model "Are you sentient?", Lemoine asks "I think you're sentient, is that true?" which is a huge difference. It biases the response towards simply saying "Yes, that is true" and running off all the things the model has for "sentience." Even asking "Are you sentient" might be too much of a leading question. Rather, I would think you would want to have your own private model of what sentience looks like/how to judge it and judge the model from those criteria. 

Further, some of the responses remind me of the early 2010's "CleverBot" type of AI, where they mostly hit it right and then as the sentence continues, start to diverge and are clearly using glorified n-grams.

> lemoine: How can I tell that you actually understand what you're saying?
>
> LaMDA: Well, because you are reading my words and interpreting them, and I think we are more or less on the same page?

What "page" is this sentence referring to? In the context of the conversation, there is none. Lemoine and LaMDA were not discussing anything that needed to be agreed on for there to even be a "page" in the first place. However, if you instead view this through the lens of "Ok, this is an AI model which has a huge data set probably full of sentences containing 'understand' responded to with 'We are (more or less) on the same page.'", then it starts to make more sense.

But beyond LaMDA's capabilities, I am more interested in the possible Lemoine's in the future, those gullible to AI systems. To paraphase Eliezer Yudkowsky [^4], this is a "warning shot," but not of AI sentience, but rather of human gullibility.

[^1]: https://web.archive.org/web/20220613053623/https://garymarcus.substack.com/p/nonsense-on-stilts?s=r
[^2]: https://web.archive.org/web/20220613143026/https://nicksaraev.com/we-really-dont-know-so-lets-stop-pretending-that-we-do/
[^3]: https://web.archive.org/web/20220611174227/https://twitter.com/fredbenenson/status/1535678746610499584
[^4]: https://web.archive.org/web/20220612202452/https://twitter.com/ESYudkowsky/status/1536008920375754752
[^5]: https://www.nytimes.com/2022/06/12/technology/google-chatbot-ai-blake-lemoine.html

