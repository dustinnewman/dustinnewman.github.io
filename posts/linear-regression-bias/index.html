<!doctype html><html><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Dustin Newman's blog"><link rel="shortcut icon" type=image/x-icon href=https://dustinnewman.net/favicon.ico><link rel=stylesheet href=https://dustinnewman.net/css/normalize.css><link rel=stylesheet href=https://dustinnewman.net/css/magick.css><script defer language=javascript type=text/javascript src=https://dustinnewman.net/js/codelines.js></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script>MathJax={tex:{displayMath:[["$$","$$"]],inlineMath:[["$","$"]]}}</script><title>Linear Regression Bias</title></head><body><main><header><h1><a href=https://dustinnewman.net/>Dustin Newman</a></h1><nav><ul><li><a href=/ title=Posts>Posts</a></li><li><a href=/about/ title=About>About</a></li></ul></nav></header><article><h1>Linear Regression Bias</h1><p><em>May 24, 2024</em></p><p>This post aims to show the impact and importance of the bias term in linear regression models. Linear regression models take the form</p>$$
\hat{y} = \beta_{0} + \sum_{j=1}^{p} \beta_{j} X_{j}
$$<p>for inputs $X_{1}, \ldots, X_{p}$ (i.e. there are $p$ features).</p><p>I will be using the <a href=https://www.kaggle.com/datasets/yasserh/housing-prices-dataset/data>Kaggle housing prices dataset</a> as a simple example, despite the collinearity of the data. We&rsquo;ll start by getting the data</p><pre><code class=language-python>import numpy as np
import pandas as pd

data = pd.read_csv(DATA_PATH)
</code></pre><p>Some basic preprocessing:</p><pre><code class=language-python># Transform categorical data
data = pd.get_dummies(data, drop_first=True, dtype=int)
Y = data['price']
X = data.drop('price', axis=1)

# Standardize
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)
</code></pre><p>And then we add the bias term to $X$</p><pre><code class=language-python>X['bias'] = 1
X, Y = X.to_numpy(), Y.to_numpy()
</code></pre><p>Now we will compute the coefficients $\beta$ analytically using the Normal Equation</p>$$
\beta = (X^{T}X)^{-1} X^{T} y
$$<p>and then use the coefficient vector to calculate $\hat{y}$ and the mean squared error</p><pre><code class=language-python>beta = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(Y)
Yhat = X.dot(beta)
mse = np.mean((Yhat - Y) ** 2)
</code></pre><p>Now, we will see what happens when we follow the same procedure but without adding the bias term. All other steps remain the same.</p><table><thead><tr><th>Coefficient</th><th>With bias</th><th>Without bias</th></tr></thead><tbody><tr><td>Area</td><td>529330.60363747</td><td>529330.60363747</td></tr><tr><td>Bedrooms</td><td>84642.78885355</td><td>84642.78885355</td></tr><tr><td>Bathrooms</td><td>495817.70908537</td><td>495817.70908537</td></tr><tr><td>Stories</td><td>390748.2656748</td><td>390748.2656748</td></tr><tr><td>Parking</td><td>238532.39119855</td><td>238532.39119855</td></tr><tr><td>Main road</td><td>146735.42974425</td><td>146735.42974425</td></tr><tr><td>Guest room</td><td>114950.32935761</td><td>114950.32935761</td></tr><tr><td>Basement</td><td>167040.77163767</td><td>167040.77163767</td></tr><tr><td>Hot water</td><td>178965.10323907</td><td>178965.10323907</td></tr><tr><td>A/C</td><td>401991.91011608</td><td>401991.91011608</td></tr><tr><td>Preferred area</td><td>276197.74367233</td><td>276197.74367233</td></tr><tr><td>Semi-furnished</td><td>-22847.00683726</td><td>-22847.00683726</td></tr><tr><td>Unfurnished</td><td>-192857.24250658</td><td>-192857.24250658</td></tr><tr><td>Bias</td><td>4766729.24770643</td><td><em>N/A</em></td></tr></tbody></table><p>And we can see the significant improvement in the mean squared error:</p><table><thead><tr><th></th><th>With bias</th><th>Without bias</th></tr></thead><tbody><tr><td>Mean squared error</td><td>1111187722284.4001</td><td>23832895443224.234</td></tr></tbody></table><p>A 21x improvement! The plot comparing the two is even more enlightening.</p><p><img src=plot.png alt="A graph plotting the biased versus unbiased predictions for the dataset"></p><p>You can literally see the intercept term at work &ldquo;lifting&rdquo; the predictions and why it makes such a simple, yet impressive difference in the performance of the model.</p></article></main><footer></footer></body></html>