<!doctype html><html><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Dustin Newman's blog"><link rel="shortcut icon" type=image/x-icon href=https://dustinnewman.net/favicon.ico><link rel=stylesheet href=/css/style.min.css><title>Don't Blame C Part I</title></head><body><header><h2><a href=https://dustinnewman.net>Dustin Newman</a></h2><nav><ul><li><a href=/ title=posts>posts</a></li><li><a href=/about/ title=about>about</a></li></ul></nav></header><main><article><header><h1>Don't Blame C Part I</h1><div><time>July 21, 2021</time></div></header><p>Yet another <a href=https://www.openwall.com/lists/oss-security/2021/07/20/1>security vulnerability</a> has been disclosed in the Linux kernel, this one dealing with gaining root access from a user land program. The root cause is a <code>size_t</code> argument being passed as a (signed) <code>int</code>. This has people maligning C for being unsafe, unstable, and unreliable, all of which are true. But I feel compelled to defend the elderly in all things, especially when they are programming languages, and so I will do so here. Your logic may vary.</p><p>The way I see it, a parameter named &ldquo;buflen&rdquo; should <strong>never</strong>, <em>ever</em>, <strong>ever</strong> have a signed type. You hear that, Linus? Better get grepping, you old Finn. Why should it? Length can never be a negative value, only zero. So why would any sane API accept -1 as a valid length? Of course, when I say it in such plain terms this may seem obvious. I&rsquo;m sure the writer of this phony API would never agree to a statement like &ldquo;length can be negative.&rdquo; And yet… that is precisely what you are saying when you write:</p><pre><code class=language-c>char *dentry_path(struct dentry *dentry, char *buf, int buflen)
</code></pre><p>Ah, god, it hurts my eyes to just look at this. I advise you to scroll away for your own posterity, dear reader. Such affronts belong not in a civilized society. Of course, the correct way to write this is with <code>size_t buflen</code> and then we wouldn&rsquo;t be having this issue and then Twitter wouldn&rsquo;t be collectively jumping around a bonfire burning the limbs of C in an effigy and then I could get some sleep at night. Well, not since I got this strange doll from the thrift store.</p><p>This should be standard in every embedded or kernel development interview. I guess I blame… people teaching C. Why does every C tutorial and even published book start with <code>int x</code> when you never see negative values assigned? For example, K&R, perhaps <em>the</em> standard C reference, has on page 18:</p><pre><code class=language-c>long nc;
nc = 0;
while (getchar() != EOF)
    ++nc;
</code></pre><p>How baffling that K&R thought using the slightly more efficient pre-increment operator is more important than using the correct type. Especially to beginners. <code>nc</code> has no business being <code>long</code>. I get it: &ldquo;int&rdquo; is fewer characters and you don&rsquo;t want to scare away beginners using incantations like &ldquo;unsigned&rdquo; or &ldquo;size underscore t.&rdquo; Everybody already knows what an &ldquo;integer&rdquo; is, or has at least heard of one. However, I think C instructors are forgetting one key fact:</p><p>A hostage audience.</p><p>No one learning C really has a choice. Either they&rsquo;re taking CS 101 or they&rsquo;re job hunting and want an extra tag in the &ldquo;skills&rdquo; box. Since you have them between a rock and a hard place, you can kind of teach them whatever you want. I&rsquo;m not suggesting we start with <code>char (*(*x())[5])()</code> but maybe getting it in early that &ldquo;types have meanings&rdquo; would be a good thing. Maybe, just maybe, it would prevent issues like the Linux kernel bug from being written in the first place. Maybe we could even turn the tide against &ldquo;int as default&rdquo; which is easily one of the worst trends in computer science.</p><p>Of course, it&rsquo;s much easier to blame C though. Dang nabbit you, Dennis Ritchie!</p></article></main><footer></footer></body></html>