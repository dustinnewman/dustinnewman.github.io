<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Tech, Frontend on Dustin Newman</title><link>https://dustinnewman.net/tags/tech-frontend/</link><description>Recent content in Tech, Frontend on Dustin Newman</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 16 Dec 2019 12:00:00 -0700</lastBuildDate><atom:link href="https://dustinnewman.net/tags/tech-frontend/index.xml" rel="self" type="application/rss+xml"/><item><title>Journey With GANs</title><link>https://dustinnewman.net/posts/journey-with-gans/</link><pubDate>Tue, 18 Jun 2024 12:35:36 -0700</pubDate><guid>https://dustinnewman.net/posts/journey-with-gans/</guid><description>&lt;p>My first foray into generative models begins with generative adversarial models (or &lt;strong>GANs&lt;/strong>). Introduced in 2014 by Ian Goodfellow et al. &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>, GANs quickly gained popularity for their ability to generate high quality, diverse, high-dimensional data samples. As you will soon see, however, the emphasis here is very much on &amp;ldquo;ability to,&amp;rdquo; and not &amp;ldquo;tendency to,&amp;rdquo; for GANs are notoriously sensitive to hyperparameters, often become unstable during training due to their adversarial nature, and are prone to vanishing gradients. Luckily, the generative capability of these models inspired an equally impressive volume of research and experimentation on how to mitigate these challenges and improve the performance of GANs for some truly amazing results.&lt;/p>
&lt;p>For all experiments here (except &lt;a href="#shellgan">ShellGAN&lt;/a>), I used the &lt;a href="https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html">CelebA&lt;/a> dataset: 200k 178 x 218 pixels images of celebrities. The full dataset is labelled, but I was only interested in unsupervised learning here.&lt;/p>
&lt;h2 id="gan-overview">GAN Overview&lt;/h2>
&lt;p>GANs are composed of two models: the generator model and the discriminator model. The generator &lt;em>generates&lt;/em> samples from the underlying distribution of the training data, while the discriminator &lt;em>discriminates&lt;/em> between real samples from the training data and fake samples produced by the generator. The basic idea is to train the two models simultaneously, so that both improve together at similar rates. The training is adversarial because the generator&amp;rsquo;s goal is to fool the discriminator with convincing fake samples, while the discriminator&amp;rsquo;s goal is to determine which samples are from the data and which are generated. Ian Goodfellow uses the following analogy:&lt;/p>
&lt;blockquote>
&lt;p>The generative model can be thought of as analogous to a team of counterfeiters, trying to produce fake currency and use it without detection, while the discriminative model is analogous to the police, trying to detect the counterfeit currency. Competition in this game drives both teams to improve their methods until the counterfeits are indistiguishable from the genuine articles.&lt;/p>
&lt;/blockquote>
&lt;p>Training for the overall GAN is therefore framed as a &amp;ldquo;game&amp;rdquo; between the generator and the discriminator. The generator&amp;rsquo;s objective is to minimize the discriminator&amp;rsquo;s score (by fooling it) and the discriminator&amp;rsquo;s objective is maximize its probability of correctly classifying the real and fake data. Assuming a class label of 1 for real data and 0 for fake data, the value function is&lt;/p>
$$
\underset{G}{\min}\underset{D}{\max} \mathbb{E}_{x \sim p_{X}}[\log D(x)] + \mathbb{E}_{z \sim p_{Z}}[\log (1 - D(G(z)))]
$$
&lt;p>where $p_{X}$ is the probability distribution of the training data and $p_{Z}$ is the probability distribution of the latent space (usually the uniform distribution). Walking through this, the value function is maximal when $D(x) = 1$ (because $\log D(x) = \log 1 = 0$) and $D(G(z)) = 0$ and it is minimal when $D(G(z)) = 1$ (because $\log (1 - D(G(z))) = -\infty$). So you can see the adversarial relationship is reflected here. In terms of training, the basic formulation is&lt;/p>
&lt;pre>&lt;code class="language-python">for real_images in data:
z = get_noise_vector()
fake_images = generator(z)
fake_predictions = discriminator(fake_images)
real_predictions = discriminator(real_images)
loss_d = BCELoss(fake_predictions, 0) + BCELoss(real_predictions, 1)
loss_g = BCELoss(fake_predictions, 1)
&lt;/code>&lt;/pre>
&lt;p>I did not actually implement the original GAN architecture using fully-connected layers from this paper on the CelebA dataset since, by the time of writing this, several iterations of improvements are already standard and produce much better results. However, it is actually quite instructive to use this architecture on simpler datasets to get an intuition for some of the dynamics and results being produced here. I first trained a simple two fully-connected layer GAN using a training set of 10,000 samples from the unit normal distribution (mean 0, standard deviation 1) with a 10,000 dimension noise vector for 10,000 epochs. I want to show the progression of generated data first so you can see what the gist is.&lt;/p>
&lt;video width="100%" height="auto" loop mute controls playsinline>
&lt;source src="./normgan_distribution_dark.mp4" type="video/mp4" />
&lt;/video>
&lt;p>You can see that the generated data starts off centered at zero and then quickly begins oscillating around the mean, coming back towards the mean and then back around again for a couple thousand epochs. However, then around epoch 3,000 the generator starts matching the training distribution rather accurately. I like this GIF because it really captures the &amp;ldquo;game&amp;rdquo; aspect of GAN training. After around epoch 7,000, the generated data starts doing a little dance and cheer, oscillating slightly around the mean. The generator and discriminator really are playing a game, with one gaining the upper-hand slightly and the other reacting quickly thereafter. Matching up the epochs show above with the loss dynamics is also illuminating.&lt;/p>
&lt;p>&lt;img src="normgan_loss_dark.png" alt="Loss functions of the generator and discriminator in vanilla GAN">&lt;/p>
&lt;p>Again, the adversarial aspects of the generator and discriminator are reflected in the loss functions, almost looking like a sin-cosine wave pair in the beginning. When the generator starts improving and generating better fakes, the discriminator&amp;rsquo;s loss goes up. It then improves via gradient-descent, reducing its loss until the point where the generator starts improving, and so on until the two reach convergence. Note that convergence in GANs means the generator and discriminator each converge to their own separate loss value, not the same loss value. Correlating from earlier, just around epoch 3,000 was when the generator started performing noticeably better to the human eye, and sure enough, the loss function reflects that, converging right around epoch 3,000. And then when the distribution starts oscillating and &amp;ldquo;cheering&amp;rdquo; around epoch 7,000, you can see the norm of the gradients starts increasing, causing larger updates to the model weights, causing larger oscillations.&lt;/p>
&lt;p>An interesting point to note that underscores GANs&amp;rsquo; instability is that the model actually starts performing &lt;em>worse&lt;/em> after reaching a optimum around epoch 6,000. After that, the gradient updates increase and both the generator and discriminator models start having higher highs and lower lows in their loss scores.&lt;/p>
&lt;h2 id="dcgan">DCGAN&lt;/h2>
&lt;p>&lt;strong>Training time&lt;/strong>: A few hours&lt;/p>
&lt;p>The deep convolutional GAN (&lt;strong>DCGAN&lt;/strong>) is where I actually started my experiments. This architecture was first introduced in 2015 &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup> and the key insight here was to use only convolutional layers and batch normalization, without any fully connected or static pooling layers. The intelligent use of convolutions proposed allows the model to better capture spatial information from the data, which it can use to generate more realistic images. Batch normalization helps to stabilize the generator and discriminator networks by reducing the internal covariate shift: the shift in the distribution of neuron activations during training. I also normalized and center-cropped the dataset as part of pre-processing which helps create more uniform contours in the cost function space and thus leads to faster and more stable convergence. With the exception of the number of feature maps, I used the same hyperparameters recommended by the paper authors.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Hyperparameter&lt;/th>
&lt;th>Value&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Learning rate $\alpha$&lt;/td>
&lt;td>0.0002&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Batch size&lt;/td>
&lt;td>128&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LeakyReLU slope&lt;/td>
&lt;td>0.2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Momentum $\beta_{1}$&lt;/td>
&lt;td>0.5&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$Z$ dimension&lt;/td>
&lt;td>100&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Convolutional layers&lt;/td>
&lt;td>4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Generator feature maps&lt;/td>
&lt;td>100&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Discriminator feature maps&lt;/td>
&lt;td>100&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Image size&lt;/td>
&lt;td>64&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>As mentioned above, I used the model architecture described by the paper authors for this.&lt;/p>
&lt;pre>&lt;code class="language-python">generator = nn.Sequential(
# Conv 1
nn.ConvTranspose2d(z_dim, ngf * 8, 4, 1, 0, bias=False),
nn.BatchNorm2d(ngf * 8),
nn.ReLU(inplace = True),
# Conv 2
nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),
nn.BatchNorm2d(ngf * 4),
nn.ReLU(inplace = True),
# Conv 3
nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),
nn.BatchNorm2d(ngf * 2),
nn.ReLU(inplace = True),
# Conv 4
nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),
nn.BatchNorm2d(ngf),
nn.ReLU(inplace = True),
# Conv 5
nn.ConvTranspose2d(ngf, IM_CHAN, 4, 2, 1, bias=False),
nn.Tanh()
)
discriminator = nn.Sequential(
# Conv 1
nn.Conv2d(IM_CHAN, ndf, 4, 2, 1),
nn.BatchNorm2d(ndf),
nn.LeakyReLU(0.2, inplace=True),
# Conv 2
nn.Conv2d(ndf, ndf * 2, 4, 2, 1),
nn.BatchNorm2d(ndf * 2),
nn.LeakyReLU(0.2, inplace=True),
# Conv 3
nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1),
nn.BatchNorm2d(ndf * 4),
nn.LeakyReLU(0.2, inplace=True),
# Conv 4
nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1),
nn.BatchNorm2d(ndf * 8),
nn.LeakyReLU(0.2, inplace=True),
# Conv 5
nn.Conv2d(ndf * 8, 1, 4, 1, 0),
nn.Sigmoid(),
nn.Flatten(),
)
&lt;/code>&lt;/pre>
&lt;p>After training for 50 epochs with a batch size of 128 images per batch, the results were&amp;hellip; not awful.&lt;/p>
&lt;p>&lt;img src="dcgan_1.png" alt="A grid of 16 blurry vaguely face-looking images">&lt;/p>
&lt;p>But they certainly weren&amp;rsquo;t good either. At best, they were interesting. I could see the generator was learning probably 70-75% of what made a face, but failing to make a convincing final composition. And this is despite the fact that training actually went exactly what you would expect. Since the discriminator has an &amp;ldquo;easier job&amp;rdquo; than the generator, it is common for the discriminator loss to progressively lessen towards 0 as the generator loss converges to some positive number. Indeed, this is exactly what I observed.&lt;/p>
&lt;p>&lt;img src="dcgan_loss_1.png" alt="A graph showing the discriminator loss converging to zero and the generator loss gradually increasing around three">&lt;/p>
&lt;p>Because the scales for the generator and discriminator loss are often quite different in GANs (with the generator having the higher loss), it is a bit more enlightening to rather look at the normalized losses to see the shape and direction of the loss functions rather than the exact values. You can also definitely see the &amp;ldquo;adversarial&amp;rdquo; aspect more on display here with the inverse correlation in the loss functions.&lt;/p>
&lt;p>&lt;img src="dcgan_loss_1_norm.png" alt="A graph showing the normalized discriminator loss decreasing and the normalized generator loss increasing">&lt;/p>
&lt;p>This is basically what you expect. The discriminator gets better and better and, as it does, the generator&amp;rsquo;s loss rises. As long as the generator and discriminator learn &lt;em>together&lt;/em> at a similar pace, learning overall still occurs. However, if the discriminator becomes too good at determining fakes, it quickly starts scoring a near perfect score and giving the generator little &amp;ldquo;feedback&amp;rdquo;. This is known as the &lt;strong>vanishing gradient&lt;/strong> problem. Remember that the discriminator outputs a sigmoid score which is the probability the discriminator is assigning of the image being fake (0) or real (1). So, as the discriminator gets better and better, its probabilities get pushed further and further towards either 0 or 1, resulting in progressively less feedback being passed back to the generator for it to improve.&lt;/p>
&lt;p>&lt;img src="sigmoid.png" alt="A graph of the sigmoid function with the tails highlighted as vanishing gradients and areas of zero derivative.">&lt;/p>
&lt;p>Once the vanishing gradient problem has taken root, no additional epochs will help the generator since further steps of gradient descent will only incrementally push the weights of the discriminator closer to its (local) optimum. And because the generator cannot improve, the discriminator stops learning as well.&lt;/p>
&lt;h3 id="one-layer-deeper">One layer deeper&lt;/h3>
&lt;p>&lt;strong>Training time&lt;/strong>: A few hours&lt;/p>
&lt;p>After these middling results, I was figuring I was simply not using a large or deep enough architecture. Faces have quite a lot of features to learn and I wanted to see if the results improved if I gave the generator in particular bigger feature maps (convolutional output channels) to better learn spatial information from the data, coupled with bigger image sizes. I made the following changes to my architecture:&lt;/p>
&lt;ul>
&lt;li>Increase image size from 64 to 128 (more data = better model?)&lt;/li>
&lt;li>Add extra convolutional layer in both generator and discriminator&lt;/li>
&lt;li>Decrease batch size from 128 to 64&lt;/li>
&lt;li>Increase generator feature map count &lt;code>ngf&lt;/code> to 160&lt;/li>
&lt;li>Decrease discriminator feature map count &lt;code>ndf&lt;/code> to 40&lt;/li>
&lt;/ul>
&lt;p>The training time was similar to above with 50 epochs and the same dataset (just larger). After training, we see an increase in image quality but not in overall fidelity. That is, there are more details but the details are nightmarishly wrong. Unfortunately, there were similar vanishing gradient problems as earlier. This makes sense, as there were no changes to the loss function to help mitigate this. Various methods for this are known, such as Wasserstein GAN with gradient penalty (WGAN-GP). &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>&lt;/p>
&lt;p>&lt;img src="dcgan_2.png" alt="A grid of 16 more detailed but more uncanny faces">&lt;/p>
&lt;h2 id="stylegan2">StyleGAN2&lt;/h2>
&lt;p>&lt;strong>Training time&lt;/strong>: 14 hours&lt;/p>
&lt;p>For my next step, eager to get SOTA results, I actually skipped past WGAN and wanted to try my hand at the latest and greatest in GANs: StyleGAN2 (which uses WGAN-GP loss). &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>&lt;/p>
&lt;p>For running StyleGAN2, my M1 MacBook Pro simply wouldn&amp;rsquo;t cut it. I needed real GPU power, the least of which for CUDA support to actually run &lt;a href="https://github.com/NVlabs/stylegan2-ada-pytorch">Nvidia&amp;rsquo;s implementation of StyleGAN2&lt;/a>. I used the implemention with adaptive instance normalization (&lt;code>AdaIN&lt;/code>) for even better results. &lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup> All GPUs used for this post were rented from &lt;a href="https://vast.ai">Vast.ai&lt;/a>. I initially had some trouble since I was using the default image &lt;code>pytorch/pytorch:latest&lt;/code> instead of the image specifically with CUDA/NVCC support &lt;code>pytorch/pytorch:2.2.0-cuda12.1-cudnn8-devel&lt;/code>. You will notice that both images (at least at the time of writing) use PyTorch &lt;strong>2&lt;/strong> where Nvidia&amp;rsquo;s implementation uses PyTorch 1.7.1. Unfortunately, this proved to be an issue for me, as I could not get Nvidia&amp;rsquo;s mainline to work. Fortunately, there was &lt;a href="https://github.com/woctezuma/stylegan2-ada-pytorch.git">a fork&lt;/a> to add support for PyTorch 2 from Github user &lt;code>woctezuma&lt;/code>. Thank you, &lt;code>woctezuma&lt;/code>.&lt;/p>
&lt;p>Even after sorting the runtime and environment issues out, however, all was not well. Due to my several attempts at training at this point, I had caused the Google Drive host for the CelebA dataset to reach its quota for 24 hours. After waiting until it was freed up again, now &lt;code>torchvision&lt;/code> was causing some issue (likely due to the version on my instance being outdated) where the Google Drive response would not be the data, but rather a virus scan page stating that Google could not verify the integrity of the data and asking if I wanted to proceed.&lt;/p>
&lt;pre>&lt;code>/opt/conda/lib/python3.10/site-packages/torchvision/datasets/utils.py:260:
UserWarning: We detected some HTML elements in the downloaded file. This most likely means that the download triggered an unhandled API response by GDrive.
Please report this to torchvision at https://github.com/pytorch/vision/issues including the response:
&lt;/code>&lt;/pre>
&lt;p>Unfortunately, &lt;code>torchvision&lt;/code> offered no support to accept this prompt, so I had to upload my local copy of CelebA to the instance, which added an extra 30 minutes to my rental time.&lt;/p>
&lt;p>This issue really highlights some of the specific challenges with ML workloads that are not faced in typical programming: it is not only that the computation required is many times more expensive, but also that the data sizes involved are also larger (thus running into network bandwidth limitations). This breaks the usual classification of programs into either &amp;ldquo;I/O-bound&amp;rdquo; or &amp;ldquo;compute-bound&amp;rdquo; processes, since ML training is both! (Not to even mention storage-bound.) And because there is a very real chance of exhausting system resources, this makes frequent check-pointing all the more important. You should checkpoint any and all information needed to restart without issue: epoch, step, loss history, and of course the model weights themselves.&lt;/p>
&lt;p>After running for 5,000 kimg and 20.5 hours, the (cherry-picked) results are below.&lt;/p>
&lt;p>&lt;img src="stylegan2.jpg" alt="A grid of 16 celebrity faces">&lt;/p>
&lt;p>The countless optimizations that make up StyleGAN2 are too extensive to relate here, but if you are interested, I strongly recommend the paper. With a training time of 20.5 hours and at a GPU rental cost of \$0.475 per hour, the total cost for training this round was \$9.74. Not bad!&lt;/p>
&lt;h2 id="shellgan">ShellGAN&lt;/h2>
&lt;p>&lt;strong>Time&lt;/strong>: 1 week&lt;/p>
&lt;p>After my initial experiments with CelebA, I was still left somewhat disappointed that I wasn&amp;rsquo;t getting the astounding, hyperrealistic images we see from SOTA diffusion models today. Sure, &lt;a href="https://thispersondoesnotexist.com">thispersondoesnotexist.com&lt;/a> could do a pretty amazing job, being bankrolled by Nvidia and unimaginably more computing resources than I could ever afford. But was there a way to use GANs as I had the resources to train and generate truly high fidelity, original images? My inspiration came after actually looking at some samples from the CelebA dataset and realizing that it - and human faces in general - were rather ambitious. The images were extremely diverse in poses, angles, backgrounds, and lighting; getting GANs in particular to stablize on something with as many easily identifiable features as the face was tricky; and humans literally have &lt;a href="https://en.wikipedia.org/wiki/Fusiform_face_area">specialized neural hardware&lt;/a> just for recognizing faces, so we are extremely discerning on an instinctual level. That was when I came across a &lt;a href="https://nicknewberg.com/GAN-explorations">GAN showcase&lt;/a> trained on - among other things - sea shells. Sea shells were the perfect solution to the issues I was seeing. They were detailed but rather abstract, with human evaluators being a lot less likely to immediately notice whether a certain shell was &amp;ldquo;off.&amp;rdquo; However, they weren&amp;rsquo;t boring blobs either. Shells are still beautiful and pleasant to look at. Luckily, I was able to find and collect a dataset which also eliminated some of the problems I had with CelebA: uniform posing and lighting with very limited background diversity.&lt;/p>
&lt;h3 id="data">Data&lt;/h3>
&lt;p>I found it helpful to, instead of scraping the site, traversing and downloading as you go (like a human would), to rather scrape and collect only the image sources (URLs) into a file for later use. This way you have more control over how you want to download (backoffs, naming schemes, a pre-known idea of how many files you will be downloading) as well as making it lower stakes for the scraping process itself, since if you make a mistake in your webscraper, you don&amp;rsquo;t need to repeat downloading images. I had to learn this through trial-and-error, but it was extremely beneficial to use &lt;code>try-except&lt;/code> blocks to continue most work even if a few pages had an unexpected format or network issues. You can then note all failed pages or image downloads in a separate log for analysis and retry. If I had to extract these lessons out to a few general guidelines, they would be:&lt;/p>
&lt;ul>
&lt;li>Log program state in files frequently&lt;/li>
&lt;li>Handle errors gracefully and diligently&lt;/li>
&lt;li>Separate out &amp;ldquo;pre-scraping&amp;rdquo; metadata and actual data downloading&lt;/li>
&lt;/ul>
&lt;h3 id="training">Training&lt;/h3>
&lt;p>I ran the training loop for 4,600 kimg for 25 hours using 1 Nvidia RTX 3090 GPU with 72 GB of disk storage for the considerable dataset size. Seeing as my raw dataset was 300 x 300 pixel images, it was quite natural to rescale to 256 pixels. However, I did have to truncate the full dataset slightly from 202,736 images to only 200,000 due to GPU RAM constraints. Given the scale of the dataset, this was acceptable. Learning from my previous experience training on CelebA, I made sure to allocate adequate resources for this run, in both memory and disk space. You might have noticed that I terminated slightly earlier than the 5,000 kimg mentioned by Nvidia as a reasonable stopping point. This is because, as I suspected, the shell images were far easier to get &amp;ldquo;reasonable&amp;rdquo; results from than Nvidia&amp;rsquo;s facial datasets, so I was already more than satisfied by the 4,600 kimg mark.&lt;/p>
&lt;h3 id="results">Results&lt;/h3>
&lt;p>After a long string of questionable results with GANs on CelebA, I am pleased to say that &amp;ldquo;ShellGAN&amp;rdquo; performed exactly as envisioned. These are the results picked from the 4,600 kimg tick.&lt;/p>
&lt;p>&lt;img src="shellgan.jpg" alt="Grid of generated shell images">&lt;/p>
&lt;p>And some additional seed generation close-ups. Unfortunately, I wasn&amp;rsquo;t able to reproduce a green shell, which was my favorite color from the results.&lt;/p>
&lt;figure>
&lt;img src="./shellgan_seed0078.png">
&lt;figcaption>Seed 78&lt;/figcaption>
&lt;/figure>
&lt;figure>
&lt;img src="./shellgan_seed0375.png">
&lt;figcaption>Seed 375&lt;/figcaption>
&lt;/figure>
&lt;figure>
&lt;img src="./shellgan_seed21313.png">
&lt;figcaption>Seed 21313&lt;/figcaption>
&lt;/figure>
&lt;p>Nvidia also provided some really cool, easy to use projection capabilities in their StyleGAN2 repo. Projection is taking a given input image and finding some latent space vector which, when passed through the generator, produces a similar image according to some metric. For this implementation, Nvidia chose to use VGG16 perceptual loss, which is a standard convolutional neural network (CNN) architecture that gained a lot of popularity after release. Given some image $I$, we want to find a latent space vector $w$ such that&lt;/p>
$$
w = \arg \min_{\mathbf{w^*}} \mathcal{L}(G(\mathbf{w^*}), I)
$$
&lt;p>I chose an image of a frog because it is roughly the same shape and dimensions as a shell (short, round), with some textured skin and somewhat natural coloring. Also, as described earlier, the green shells were my favorite so I wanted more of those. Below is the progress video of the optimization process, trying to find the $w$ vector and display $G(w)$ at each step (out of 1,000).&lt;/p>
&lt;video width="100%" height="auto" loop mute controls playsinline>
&lt;source src="./projection_frog.mp4" type="video/mp4" />
&lt;/video>
&lt;p>I also tried an eye for similar reasons. It also has a similar black background with one object in the center focus.&lt;/p>
&lt;video width="100%" height="auto" loop mute controls playsinline>
&lt;source src="./projection_eye.mp4" type="video/mp4" />
&lt;/video>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>As my first foray into generative models, these experiments were not just highly enlightening but really fun. It is an amazing blend of technical challenges with immediately interpretable visual results which is exciting, especially to motivate you through numerous PyTorch or CUDA errors. As amazing as these results were, however, GANs are still really only the first step in the generative model journey for me. Next, I will be experimenting with diffusion models which have produced some astounding results and address many of the problems from GANs.&lt;/p>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>Goodfellow, Ian J., Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. &amp;ldquo;Generative adversarial nets&amp;rdquo;. &lt;em>Advances in neural information processing systems&lt;/em> 27 (2014). &lt;a href="https://arxiv.org/pdf/1406.2661">https://arxiv.org/pdf/1406.2661&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>Radford, Alec, Luke Metz, and Soumith Chintala. &amp;ldquo;Unsupervised representation learning with deep convolutional generative adversarial networks.&amp;rdquo; &lt;em>arXiv preprint arXiv:1511.06434&lt;/em> (2015). &lt;a href="https://arxiv.org/pdf/1511.06434">https://arxiv.org/pdf/1511.06434&lt;/a>&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3">
&lt;p>Arjovsky, Martin, Soumith Chintala, Léon Bottou. &amp;ldquo;Wasserstein GAN.&amp;rdquo; &lt;em>International conference on machine learning&lt;/em> (2017). &lt;a href="https://arxiv.org/pdf/1701.07875">https://arxiv.org/pdf/1701.07875&lt;/a>&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4">
&lt;p>Karras, Tero, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. &amp;ldquo;Analyzing and improving the image quality of stylegan.&amp;rdquo; &lt;em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition&lt;/em> (2020). &lt;a href="https://arxiv.org/pdf/1912.04958">https://arxiv.org/pdf/1912.04958&lt;/a>&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5">
&lt;p>Huang, Xun, and Serge Belongie. &amp;ldquo;Arbitrary style transfer in real-time with adaptive instance normalization.&amp;rdquo; &lt;em>Proceedings of the IEEE international conference on computer vision&lt;/em> (2017). &lt;a href="https://arxiv.org/pdf/1703.06868">https://arxiv.org/pdf/1703.06868&lt;/a>&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>Remove Duplicates 2</title><link>https://dustinnewman.net/posts/remove-duplicates-2/</link><pubDate>Mon, 17 Jun 2024 22:42:50 -0700</pubDate><guid>https://dustinnewman.net/posts/remove-duplicates-2/</guid><description>&lt;p>I have recently been brushing up on some interview prep with LeetCode. For one of the problems, despite only being a &amp;ldquo;medium&amp;rdquo; difficulty, I came up with an interesting solution that pulled from my experience developing cellular firmware that I would like to share. It is very common when dealing with frequency ranges for radio frequency (RF) bands and that these ranges are organized in tables. It is also common for special rules or regulations to apply when dealing with the &amp;ldquo;edge&amp;rdquo; ranges of these bands. For example, if we have band 1, it might have a low and high edge that has a lower or higher maximum transmissible power.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Band&lt;/th>
&lt;th>Frequency&lt;/th>
&lt;th>Edge?&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>1&lt;/td>
&lt;td>10&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>1&lt;/td>
&lt;td>11&lt;/td>
&lt;td>No&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>1&lt;/td>
&lt;td>12&lt;/td>
&lt;td>No&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>1&lt;/td>
&lt;td>13&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2&lt;/td>
&lt;td>20&lt;/td>
&lt;td>No&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>3&lt;/td>
&lt;td>24&lt;/td>
&lt;td>No&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>A common way to identify these edge regions is to compare the current band with the previous and next bands. If both the previous and next bands are the same as the current, then we are in an edge region. Else, we aren&amp;rsquo;t. Then you can add some special handling for the first and last regions in the table, etc.&lt;/p>
&lt;p>Now, the LeetCode problem is to remove duplicates from an array of integers, but allow up to two duplicates for any given value. So if you had the array &lt;code>[1,1,1,2,2,3]&lt;/code>, the output would be &lt;code>[1,1,2,2,3]&lt;/code>. On the face of it, this has nothing to do with the frequency range concept I just described. However, I have always visualized the edge region filtering as a sort of &amp;ldquo;catching the wave.&amp;rdquo; Basically, if we turn our table into an array of only the band numbers, we can denote the start of the band with a forward slash / and the end of the band with a backwards slash. Non-edge regions will be denoted with a dash -.&lt;/p>
&lt;pre>&lt;code> / - - \ / /
[1,1,1,1,2,3]
&lt;/code>&lt;/pre>
&lt;p>Note the slight difference from the table earlier. Here, when we first see a new band number, we somewhat optimistically label it the &amp;ldquo;rising edge&amp;rdquo; of the band.&lt;/p>
&lt;p>My key insight was realizing that removing the duplicates with an allowance of two actually reduced to &amp;ldquo;catching the wave.&amp;rdquo; We want to take only the rising and falling edges of the frequency bands. This allowed me to apply both my RF knowledge and the corresponding embedded programming knowledge (where every byte counts) to come up with a quite minimal C++ solution.&lt;/p>
&lt;pre>&lt;code class="language-cpp">int removeDuplicates(vector&amp;lt;int&amp;gt;&amp;amp; nums) {
int k = 1;
for (size_t i = 1; i &amp;lt; nums.size(); i++) {
if (nums[i] != nums[i - 1] || i == nums.size() - 1 || nums[i] != nums[i + 1]) {
nums[k++] = nums[i];
}
}
return k;
}
&lt;/code>&lt;/pre>
&lt;p>where &lt;code>k&lt;/code> denotes the length of the de-duplicated array. (Don&amp;rsquo;t judge me for using a signed integer for the return value. LeetCode forced the interface on me.) We initialize it to 1 to reflect the fact that every non-empty array (LeetCode guaranteed the arrays would be non-empty) will contain at least 1 element. We then iterate through the array and &amp;ldquo;catch the waves&amp;rdquo;: if the current element is not equal to the previous or next elements, then we have either the rising or falling edges. And then there is an additional check for the final element of the array which should always be included (either because it is the falling edge of a run or because it is unique).&lt;/p></description></item><item><title>Linear Regression Bias</title><link>https://dustinnewman.net/posts/linear-regression-bias/</link><pubDate>Fri, 24 May 2024 16:25:25 -0700</pubDate><guid>https://dustinnewman.net/posts/linear-regression-bias/</guid><description>&lt;p>This post aims to show the impact and importance of the bias term in linear regression models. Linear regression models take the form&lt;/p>
$$
\hat{y} = \beta_{0} + \sum_{j=1}^{p} \beta_{j} X_{j}
$$
&lt;p>for inputs $X_{1}, \ldots, X_{p}$ (i.e. there are $p$ features).&lt;/p>
&lt;p>I will be using the &lt;a href="https://www.kaggle.com/datasets/yasserh/housing-prices-dataset/data">Kaggle housing prices dataset&lt;/a> as a simple example, despite the collinearity of the data. We&amp;rsquo;ll start by getting the data&lt;/p>
&lt;pre>&lt;code class="language-python">import numpy as np
import pandas as pd
data = pd.read_csv(DATA_PATH)
&lt;/code>&lt;/pre>
&lt;p>Some basic preprocessing:&lt;/p>
&lt;pre>&lt;code class="language-python"># Transform categorical data
data = pd.get_dummies(data, drop_first=True, dtype=int)
Y = data['price']
X = data.drop('price', axis=1)
# Standardize
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)
&lt;/code>&lt;/pre>
&lt;p>And then we add the bias term to $X$&lt;/p>
&lt;pre>&lt;code class="language-python">X['bias'] = 1
X, Y = X.to_numpy(), Y.to_numpy()
&lt;/code>&lt;/pre>
&lt;p>Now we will compute the coefficients $\beta$ analytically using the Normal Equation&lt;/p>
$$
\beta = (X^{T}X)^{-1} X^{T} y
$$
&lt;p>and then use the coefficient vector to calculate $\hat{y}$ and the mean squared error&lt;/p>
&lt;pre>&lt;code class="language-python">beta = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(Y)
Yhat = X.dot(beta)
mse = np.mean((Yhat - Y) ** 2)
&lt;/code>&lt;/pre>
&lt;p>Now, we will see what happens when we follow the same procedure but without adding the bias term. All other steps remain the same.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Coefficient&lt;/th>
&lt;th>With bias&lt;/th>
&lt;th>Without bias&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Area&lt;/td>
&lt;td>529330.60363747&lt;/td>
&lt;td>529330.60363747&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Bedrooms&lt;/td>
&lt;td>84642.78885355&lt;/td>
&lt;td>84642.78885355&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Bathrooms&lt;/td>
&lt;td>495817.70908537&lt;/td>
&lt;td>495817.70908537&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Stories&lt;/td>
&lt;td>390748.2656748&lt;/td>
&lt;td>390748.2656748&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Parking&lt;/td>
&lt;td>238532.39119855&lt;/td>
&lt;td>238532.39119855&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Main road&lt;/td>
&lt;td>146735.42974425&lt;/td>
&lt;td>146735.42974425&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Guest room&lt;/td>
&lt;td>114950.32935761&lt;/td>
&lt;td>114950.32935761&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Basement&lt;/td>
&lt;td>167040.77163767&lt;/td>
&lt;td>167040.77163767&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Hot water&lt;/td>
&lt;td>178965.10323907&lt;/td>
&lt;td>178965.10323907&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>A/C&lt;/td>
&lt;td>401991.91011608&lt;/td>
&lt;td>401991.91011608&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Preferred area&lt;/td>
&lt;td>276197.74367233&lt;/td>
&lt;td>276197.74367233&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Semi-furnished&lt;/td>
&lt;td>-22847.00683726&lt;/td>
&lt;td>-22847.00683726&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Unfurnished&lt;/td>
&lt;td>-192857.24250658&lt;/td>
&lt;td>-192857.24250658&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Bias&lt;/td>
&lt;td>4766729.24770643&lt;/td>
&lt;td>&lt;em>N/A&lt;/em>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>And we can see the significant improvement in the mean squared error:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>With bias&lt;/th>
&lt;th>Without bias&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Mean squared error&lt;/td>
&lt;td>1111187722284.4001&lt;/td>
&lt;td>23832895443224.234&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>A 21x improvement! The plot comparing the two is even more enlightening.&lt;/p>
&lt;p>&lt;img src="plot.png" alt="A graph plotting the biased versus unbiased predictions for the dataset">&lt;/p>
&lt;p>You can literally see the intercept term at work &amp;ldquo;lifting&amp;rdquo; the predictions and why it makes such a simple, yet impressive difference in the performance of the model.&lt;/p></description></item><item><title>Rust Trait Coherence</title><link>https://dustinnewman.net/posts/rust-trait-coherence/</link><pubDate>Sat, 04 Nov 2023 16:36:26 -0700</pubDate><guid>https://dustinnewman.net/posts/rust-trait-coherence/</guid><description>&lt;h2 id="the-background">The Background&lt;/h2>
&lt;p>I defined a struct in a Rust project.&lt;/p>
&lt;pre>&lt;code class="language-rust">struct Content
&lt;/code>&lt;/pre>
&lt;p>I then wanted an easy way to convert from different, other types into &lt;code>Content&lt;/code>, ideally staying within the Rust type system. My motivating example will be that you want to go from a file path to &lt;code>Content&lt;/code>. Since I am a good, active Rust user, I know that I don&amp;rsquo;t want to unnecessarily restrict my conversion function to only the concrete type &lt;code>Path&lt;/code> because the same logic applies to all sorts of &lt;code>Path&lt;/code>-like types e.g. &lt;code>PathBuf&lt;/code>, &lt;code>Components&lt;/code>, &lt;code>OsStr&lt;/code>, and even &lt;code>String&lt;/code>. So I implemented this trait generically.&lt;/p>
&lt;pre>&lt;code class="language-rust">impl&amp;lt;P&amp;gt; TryFrom&amp;lt;P&amp;gt; for Content where P: AsRef&amp;lt;Path&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>The compiler threw an error message. This in itself is not too surprising: compiler errors are the bread and butter of Rust programming. But this particular compiler error threw me off guard.&lt;/p>
&lt;pre>&lt;code class="language-rust">conflicting implementation in crate `core`:
- impl&amp;lt;T, U&amp;gt; TryFrom&amp;lt;U&amp;gt; for T
where U: Into&amp;lt;T&amp;gt;;
&lt;/code>&lt;/pre>
&lt;p>This came as a bit of a shock to me because, well, I had just defined this struct &lt;code>Content&lt;/code>. How could Rust &lt;code>core&lt;/code> already have an implementation for it? To understand, we have to unpack what the generics in this type signature are really conveying. This type signature says that we have an implementation of &lt;code>TryFrom&lt;/code> &lt;em>from&lt;/em> any type &lt;code>&amp;lt;U&amp;gt;&lt;/code> &lt;em>for&lt;/em> any type &lt;code>&amp;lt;T&amp;gt;&lt;/code> if and only if &lt;code>&amp;lt;U&amp;gt;&lt;/code> already implements &lt;code>Into&amp;lt;T&amp;gt;&lt;/code>. Ok, when you put it like that, this actually seems quite reasonable. If we already have the logic to go from &lt;code>&amp;lt;U&amp;gt;&lt;/code> to &lt;code>&amp;lt;T&amp;gt;&lt;/code> infallibly, then surely we can represent this as a &amp;ldquo;fallible&amp;rdquo; operation, which simply returns &lt;code>Ok&lt;/code> all the time. And, in fact, when you look at the &lt;a href="https://github.com/rust-lang/rust/blob/f5ca57e153afaed818f8be88abf5ce46715c0f9a/library/core/src/convert/mod.rs#L800">Rust core source code&lt;/a>, this is exactly what it does.&lt;/p>
&lt;pre>&lt;code class="language-rust">// Infallible conversions are semantically equivalent to fallible conversions
// with an uninhabited error type.
#[stable(feature = &amp;quot;try_from&amp;quot;, since = &amp;quot;1.34.0&amp;quot;)]
impl&amp;lt;T, U&amp;gt; TryFrom&amp;lt;U&amp;gt; for T
where
U: Into&amp;lt;T&amp;gt;,
{
type Error = Infallible;
#[inline]
fn try_from(value: U) -&amp;gt; Result&amp;lt;Self, Self::Error&amp;gt; {
Ok(U::into(value))
}
}
&lt;/code>&lt;/pre>
&lt;p>Put this way, this seems not just reasonable but obvious. Of course, this makes sense. The issue I still didn&amp;rsquo;t understand though was that, sure, for any type &lt;code>&amp;lt;U&amp;gt;&lt;/code> if &lt;code>&amp;lt;U&amp;gt;&lt;/code> implements &lt;code>Into&amp;lt;T&amp;gt;&lt;/code> you can use this logical, &amp;ldquo;blanket&amp;rdquo; implementation to kind of vacuously implement &lt;code>TryFrom&amp;lt;U&amp;gt; for T&lt;/code>, &lt;em>but&lt;/em> my issue was that &lt;em>my&lt;/em> &lt;code>&amp;lt;U&amp;gt;&lt;/code> (&lt;code>P: AsRef&amp;lt;Path&amp;gt;&lt;/code>) &lt;strong>doesn&amp;rsquo;t&lt;/strong> implement &lt;code>Into&amp;lt;Content&amp;gt;&lt;/code>. So, why is Rust having an issue with this? And to answer it, we have to go into Rust &lt;strong>trait coherence&lt;/strong>.&lt;/p>
&lt;h2 id="trait-coherence">Trait Coherence&lt;/h2>
&lt;p>Trait coherence is &amp;ldquo;&lt;a href="https://github.com/Ixrec/rust-orphan-rules/blob/4b2ccb102bd7c715c4dc2ec4bdeaa96c6662093c/README.md?plain=1#L9">the property that there is at most one implementation of a trait for any given type&lt;/a>.&amp;rdquo; So, for example, you cannot have:&lt;/p>
&lt;pre>&lt;code class="language-rust">struct Content;
impl Eq for Content
impl Eq for Content
&lt;/code>&lt;/pre>
&lt;p>Once again, when presented like this, this seems reasonable. Despite my confusion, I have to agree that the Rust compiler is making a sane assumption here. The issue, however, quickly becomes more complicated once you allow generics and crates (i.e. third-party code and dependencies). There are two often cited rules to help the Rust compiler enforce trait coherence.&lt;/p>
&lt;ol>
&lt;li>The Orphan Rule: Either the trait or the type for which you are implementing the trait must be local to your crate&lt;/li>
&lt;li>The Overlapping Rule: For any two types A and B implementing a trait T, A must be different from B&lt;/li>
&lt;/ol>
&lt;p>The orphan rule seems much more controversial and discussed online, but I believe my issue actually stems from the overlapping rule. Now, previously I gave an obviously wrong example to motivate why the Rust compiler even has a concept of trait coherence. But to motivate the overlapping rule, let&amp;rsquo;s give a &lt;a href="https://github.com/kennytm/rfcs/blob/a956323627bbc245dd3fe657f1dbc67060e77167/text/0000-negative-bounds.md">more realistic example&lt;/a> in order to steel-man the motivation. Let&amp;rsquo;s say that we have a trait to get the average of two numbers.&lt;/p>
&lt;pre>&lt;code class="language-rust">trait Int {}
trait Float {}
trait Average {
fn average(self, other: Self) -&amp;gt; Self;
}
&lt;/code>&lt;/pre>
&lt;p>We can implement this for &lt;code>Int&lt;/code> types:&lt;/p>
&lt;pre>&lt;code class="language-rust">impl&amp;lt;T: Int&amp;gt; Average for T {
fn average(self, other: Self) -&amp;gt; Self {
if self &amp;gt;= other {
other + (self - other) / 2
} else {
self + (other - self) / 2
}
}
}
&lt;/code>&lt;/pre>
&lt;p>And then for &lt;code>Float&lt;/code> types:&lt;/p>
&lt;pre>&lt;code class="language-rust">impl&amp;lt;T: Float&amp;gt; Average for T {
fn average(self, other: Self) -&amp;gt; Self {
self * 0.5 + other * 0.5
}
}
&lt;/code>&lt;/pre>
&lt;p>Uh-oh! Compiler error. The same as before. &lt;code>conflicting implementations of trait Average&lt;/code>. But&amp;hellip; why? We don&amp;rsquo;t really have two &lt;em>conflicting&lt;/em> implementations, as we have &lt;em>two&lt;/em> implementations. It seems as though Rust is unnecessarily strict here since there is no issue yet. Well, you see, I actually sort of underplayed the overlapping rule earlier. It really should be&lt;/p>
&lt;ol start="2">
&lt;li>The Overlapping Rule: For any two types A and B implementing a trait T, A must be &lt;strong>provably&lt;/strong> different from B &lt;strong>in every possible program&lt;/strong>&lt;/li>
&lt;/ol>
&lt;p>This is called &lt;a href="https://aturon.github.io/blog/2017/04/24/negative-chalk/">&amp;ldquo;negative reasoning&amp;rdquo;&lt;/a>. It is essentially the difference between saying &amp;ldquo;there does not exist any type which implements a trait twice &lt;em>currently&lt;/em>&amp;rdquo; and &amp;ldquo;there will &lt;em>never&lt;/em> exist any type which implements a trait twice.&amp;rdquo; Hopefully now it is more clear what the actual issue is. There is nothing specifically &lt;strong>stopping&lt;/strong> a type from implementing &lt;strong>both&lt;/strong> &lt;code>Int&lt;/code> and &lt;code>Float&lt;/code>. In the absence of this guarantee, Rust errs on the side of caution. And to Rust&amp;rsquo;s credit, this makes sense. Just look at the following, nothing seems wrong.&lt;/p>
&lt;pre>&lt;code class="language-rust">struct Real
// Is this error?
impl Int for Real
// Or this?
impl Float for Real
&lt;/code>&lt;/pre>
&lt;p>If the compiler instead decided to follow the approach I preferred (don&amp;rsquo;t throw an error until a conflict actually exists), it would make implementating a trait an error. Not just an error, but a rather arbitrary error as well. In the example above, which implementation would you consider the &amp;ldquo;bad&amp;rdquo; one? There is no deterministic answer. This is not only unexpected, but would have breaking downstream consequences: a conflict may exist within other people&amp;rsquo;s crates because of a trait conflict that does &lt;strong>not&lt;/strong> exist on your system, but &lt;strong>does&lt;/strong> exist on theirs. This would be a huge disaster for the Rust ecosystem of crates, not to mention seriously hinder Rust&amp;rsquo;s changes of mainstream adoption. Looking at it from this perspective, it actually becomes clear why the Rust compiler decides to just play it safe and stop the problem at the root: a trait definition which could, theoretically, introduce an implementation conflict. This keeps the wider Rust ecosystem safe until a robust &amp;ldquo;fix&amp;rdquo;/solution is developed.&lt;/p>
&lt;p>But back to my original issue. You might recall that, unlike the previous example, there &lt;em>was no&lt;/em> more specific, conflicting example. I just wanted to implement&lt;/p>
&lt;pre>&lt;code class="language-rust">impl&amp;lt;P&amp;gt; TryFrom&amp;lt;P&amp;gt; for Content where P: AsRef&amp;lt;Path&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>It is not so much that there exists &amp;ldquo;two implementations&amp;rdquo; for &lt;code>TryFrom for Content&lt;/code>. It is moreso that currently Rust cannot guarantee that there will never be a type which satisfies two implementations. For example, if someone were to implement &lt;code>TryFrom&amp;lt;String&amp;gt;&lt;/code>.&lt;/p>
&lt;pre>&lt;code class="language-rust">impl TryFrom&amp;lt;String&amp;gt; for Content
&lt;/code>&lt;/pre>
&lt;p>This would introduce two valid implementations: one for the generic type &lt;code>P: AsRef&amp;lt;Path&amp;gt;&lt;/code> (which &lt;code>String&lt;/code> satisfies) and another for the specific type &lt;code>String&lt;/code>. Similarly, you could imagine we have a trait to calculate the last modified time of a file path.&lt;/p>
&lt;pre>&lt;code class="language-rust">trait LastModified {
fn last_modified(&amp;amp;self) -&amp;gt; std::time::SystemTime;
}
&lt;/code>&lt;/pre>
&lt;p>For all regular files, this is simple. Just take the modified time (&lt;code>mtime&lt;/code>).&lt;/p>
&lt;pre>&lt;code class="language-rust">impl&amp;lt;P: AsRef&amp;lt;Path&amp;gt;&amp;gt; LastModified for P {
fn last_modified(&amp;amp;self) -&amp;gt; std::time::SystemTime {
fs::metadata(self.as_ref())
.and_then(|metadata| metadata.modified())
.unwrap_or(SystemTime::UNIX_EPOCH)
}
}
&lt;/code>&lt;/pre>
&lt;p>But for directories, the &lt;code>mtime&lt;/code> might not be exactly what we want. It most likely will be the last time a file or subdirectory was added or removed from the directory, rather than the last time a file or subdirectory was actually &lt;em>modified&lt;/em>. To fix this, we would have to make a special implementation just for directories.&lt;/p>
&lt;pre>&lt;code class="language-rust">impl LastModified for ReadDir {
fn last_modified(&amp;amp;self) -&amp;gt; std::time::SystemTime {
self.filter_map(Result::ok)
.map(|entry| entry.path())
.filter_map(|path| fs::metadata(&amp;amp;path).ok())
.filter_map(|metadata| metadata.modified().ok())
.max()
.unwrap_or(SystemTime::UNIX_EPOCH)
}
}
&lt;/code>&lt;/pre>
&lt;p>So which one to choose? Currently, there is no way, and doing so would require a feature called &amp;ldquo;specialization&amp;rdquo;.&lt;/p>
&lt;p>There are a few options here:&lt;/p>
&lt;ol>
&lt;li>Extend Rust to supportive &amp;ldquo;negative&amp;rdquo; trait bounds (i.e. &lt;code>P: AsRef&amp;lt;Path&amp;gt; &amp;amp;! ContentNode: TryFrom&amp;lt;P&amp;gt;&lt;/code>) to allow the compiler to guarantee one implementation per type&lt;/li>
&lt;li>Allow multiple implementations per type and design a mechanism for deciding between them (&amp;ldquo;specialization&amp;rdquo;)&lt;/li>
&lt;li>Allow users to specify a crate locality condition that this crate will never be used by any other crate and thus we can bypass this condition just between us&lt;/li>
&lt;li>Just use &lt;code>PathBuf&lt;/code> here instead of generics&lt;/li>
&lt;/ol>
&lt;p>For now, I&amp;rsquo;m sticking with number four.&lt;/p></description></item><item><title>Farsi Ezafe Trick for L1 English Speakers</title><link>https://dustinnewman.net/posts/farsi-ezafe-trick/</link><pubDate>Sat, 28 Oct 2023 12:14:06 -0700</pubDate><guid>https://dustinnewman.net/posts/farsi-ezafe-trick/</guid><description>&lt;p>I recently came across a neat trick in &lt;em>Modern Persian&lt;/em> by Narguess Farzad (2004) for an intuition on the order of adjectives and possessives in Farsi (Iranian Persian). As an L1 English speaker, it can be difficult to get an intuition for the adjective order (postpositional) and how that works with the possessives in Farsi (also postpositional).&lt;/p>
&lt;p>You simply write the English phrase as usual: left to right. Then underneath you write the Farsi translation: right to left. Because adjectives and possessives are postpositional, but Farsi is also written right to left, the two &amp;ldquo;reverse&amp;rdquo; and provide a simple way to get the correct word order in Farsi. Farzad provides the simple example:&lt;/p>
&lt;table>
&lt;tr>
&lt;td>→&lt;/td>
&lt;td>blue&lt;/td>
&lt;td>pencil&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>آبی&lt;/td>
&lt;td>مداد&lt;/td>
&lt;td>←&lt;/td>
&lt;/tr>
&lt;/table>
&lt;p>Because possessive adjectives follow attributive adjectives, this extends to genitive constructions as well:&lt;/p>
&lt;table>
&lt;tr>
&lt;td>→&lt;/td>
&lt;td>my&lt;/td>
&lt;td>brother's&lt;/td>
&lt;td>hand&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>من&lt;/td>
&lt;td>برادر&lt;/td>
&lt;td>دست&lt;/td>
&lt;td>←&lt;/td>
&lt;/tr>
&lt;/table>
&lt;p>And to multiple adjectives:&lt;/p>
&lt;table>
&lt;tr>
&lt;td>→&lt;/td>
&lt;td>cold&lt;/td>
&lt;td>dark&lt;/td>
&lt;td>night&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>سرد&lt;/td>
&lt;td>تاریک&lt;/td>
&lt;td>شب&lt;/td>
&lt;td>←&lt;/td>
&lt;/tr>
&lt;/table>
&lt;table>
&lt;tr>
&lt;td>→&lt;/td>
&lt;td>my&lt;/td>
&lt;td>Persian&lt;/td>
&lt;td>book&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>من&lt;/td>
&lt;td>فارسی&lt;/td>
&lt;td>کتاب&lt;/td>
&lt;td>←&lt;/td>
&lt;/tr>
&lt;/table>
&lt;table>
&lt;tr>
&lt;td>→&lt;/td>
&lt;td>Maryam's&lt;/td>
&lt;td>Russian&lt;/td>
&lt;td>friend&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;/td>
&lt;td>مریم&lt;/td>
&lt;td>روسی&lt;/td>
&lt;td>دوست&lt;/td>
&lt;td>←&lt;/td>
&lt;/tr>
&lt;/table></description></item><item><title>Stable Diffusion Gallery</title><link>https://dustinnewman.net/posts/stable-diffusion-gallery/</link><pubDate>Sat, 24 Sep 2022 18:17:58 -0700</pubDate><guid>https://dustinnewman.net/posts/stable-diffusion-gallery/</guid><description>&lt;p>I have been playing with &lt;a href="https://github.com/CompVis/stable-diffusion">Stable Diffusion&lt;/a> lately, as I am sure many have. I thought it would be worthwhile to publish some of my favorites along with the prompts and seeds for each image. All images have a &amp;ldquo;classifier free guidance (CFG) scale&amp;rdquo; (or simply &amp;ldquo;guidance&amp;rdquo;) value of 7.5; are 512 by 512 pixels; use the k_lms sampler; and use 50 steps. The seeds are marked after the prompt after the -S flag.&lt;/p>
&lt;figure>&lt;img src="./frog_with_crown.png"
alt="Frog wearing a crown">&lt;figcaption>
&lt;p>&lt;em>Portrait of a green frog wearing a deep red velvet robe and golden crown, oil painting by Hyacinthe Rigaud, baroque, rich&lt;/em> -S3173279521&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;figure>&lt;img src="./van_gogh_frog_close.png"
alt="Frog by Van Gogh">&lt;figcaption>
&lt;p>&lt;em>Impressionist painting of green frog with red eyes by Van Gogh, impressionist&lt;/em> -S4232807157&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;figure>&lt;img src="./van_gogh_frog_meadow.png"
alt="Frog in a meadow by Van Gogh">&lt;figcaption>
&lt;p>&lt;em>Impressionist painting of green frog with red eyes in grass meadow, by Van Gogh, impressionist&lt;/em> -S2854560348&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;figure>&lt;img src="./robed_frog.png"
alt="Frog in robe">&lt;figcaption>
&lt;p>&lt;em>Portrait of a green frog wearing a deep red velvet robe and golden crown, oil painting by Hyacinthe Rigaud, baroque, rich&lt;/em> -S1372305360&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;figure>&lt;img src="./creepy_castle.png"
alt="Creepy castle">&lt;figcaption>
&lt;p>&lt;em>a matte painting of an old stone gothic castle on a cliff over the sea at night, creepy, dark, gothic, stormy, masterpiece, artstation&lt;/em> -S4153617768&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;figure>&lt;img src="./cottage.png"
alt="Tree cottage">&lt;figcaption>
&lt;p>&lt;em>Cozy house in the woods, anime, oil painting by Josef Thoma, high resolution, cottagecore, Studio Ghibli inspired, 4k&lt;/em> -S2819016467&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;figure>&lt;img src="./sun_mosaic.png"
alt="Sun and moon mosaic">&lt;figcaption>
&lt;p>&lt;em>The sun and the moon, roman mosaic&lt;/em> -S3992813662&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;figure>&lt;img src="./ice_pillars.png"
alt="Ice pillars">&lt;figcaption>
&lt;p>&lt;em>A white ice palace with tall pillars and an ice throne in the center, by Josef Thoma, matte painting, detailed, 4k&lt;/em> -S3712115669&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;figure>&lt;img src="./plants_dystopia.png"
alt="Dystopia with plants">&lt;figcaption>
&lt;p>&lt;em>overgrown foliage overtaking tall destroyed buildings, biopunk, scenery, professional, award-winning, trending on artstation, detailed, realistic, beautiful, emotional, shiny, golden, picture, antview, close up&lt;/em> -S1483373964&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;figure>&lt;img src="./ink_brush_painting_mountains.png"
alt="Ink brush painting of mountains">&lt;figcaption>
&lt;p>&lt;em>A beautiful mountain landscape on an ancient scroll, ink brush painting, traditional Chinese, by Ma Yuan&lt;/em> -S2983311206&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;figure>&lt;img src="./chinese_snow.png"
alt="Snow covered mountains">&lt;figcaption>
&lt;p>&lt;em>snow covered mountains in chinese watercolor painting, landscape, masterpiece, ancient&lt;/em> -S3991627781&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;figure>&lt;img src="./japanese_snow.png"
alt="Snowy mountain village">&lt;figcaption>
&lt;p>&lt;em>two people in a snowy mountain village, snow falling from the sky, japanese watercolor painting, color and ink on scroll, by hokusai, landscape painting, masterpiece, ancient&lt;/em> -S2964261701&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;figure>&lt;img src="./persian_waterfall.png"
alt="Waterfall over stones">&lt;figcaption>
&lt;p>&lt;em>large persian mosque in the middle of waterfall in chinese watercolor painting, oil painting, masterpiece, aesthetic&lt;/em> -S392247870&lt;/p>
&lt;/figcaption>
&lt;/figure></description></item><item><title>Cascading Style S-Expressions</title><link>https://dustinnewman.net/posts/lisp-css/</link><pubDate>Sun, 03 Apr 2022 19:44:12 -0700</pubDate><guid>https://dustinnewman.net/posts/lisp-css/</guid><description>&lt;p>Since I first learned it, Lisp has fascinated me. Not enough to seriously use it beyond projects for college, mind you, but enough where - years later - I’m working on something completely random and the thought crosses my mind “It would be fun to implement this in Lisp,” especially using + as a function, same as any other, in full prefix notation, something even the noble and esteemed Haskell couldn’t seem to commit to. Beyond the simple grammar and elegance, there is something about Lisp which feels fundamental to computation itself, likely the similarity to lambda calculus, which actually &lt;em>is&lt;/em> fundamental to computation itself. Sadly, Lisp is rarely used much of anywhere nowadays except as a novelty or pet project. This is strange, considering that Brendan Eich was inspired by Scheme while designing JavaScript. You would think there would be a more direct influence.&lt;/p>
&lt;p>Fortunately for us, it doesn’t have to be that way. I have long thought of writing CSS is a Lisp-style syntax. I’m not sure why exactly. Something about it just &lt;em>feels&lt;/em> right. Simple. For maximum confusion, I have decided to name this &lt;strong>Cascading Style S-Expressions&lt;/strong>: CSS for short.&lt;/p>
&lt;p>Now, I’m not gonna claim that we can bring Lisp to the web. Lisp’s acolytes are a devout and proud people, who would scoff at the claim that CSS properties and markup even come close to represent the power and beauty of Lisp. And this is correct on some level. CSS is a language for configuration, not computation. There are no functions, no inputs or outputs, no arithmetic, none of that. Just a bunch of key-value pairs.&lt;/p>
&lt;p>That said, I’m gonna do it anyways because, like I said, it’s fun. There are some parts of Lisp which fit nicely with CSS. The nesting of S-expressions is a cleaner way to combine selectors; the extremely simple syntax (the only special characters are parentheses) eschews semicolons, colons, and braces; and honestly something about CSS has just always felt Lisp-y to me. But mostly this is just for fun and an itch being scratched. The quintessential example, what every CSS tutorial starts with, is setting the background-color of the body element.&lt;/p>
&lt;pre>&lt;code>(body color red)
&lt;/code>&lt;/pre>
&lt;p>Here the first atom is the selector: the &lt;code>body&lt;/code> element. The next two atoms are actually a pair: the property (color) and its value (red). Together these make up a CSS rule which is just setting one property to some value. This is equivalent to the CSS configuration:&lt;/p>
&lt;pre>&lt;code>body {
color: red;
}
&lt;/code>&lt;/pre>
&lt;p>Can you feel the aesthetic purity already? I sure can. Multiple rules can be chained together by just repeating these pairs.&lt;/p>
&lt;pre>&lt;code>(body color red font-size 14px)
&lt;/code>&lt;/pre>
&lt;p>I’m gonna assume you already know what this would be equivalent too, so I’m not gonna write it out anymore. The real star of the show though, is nested selectors. Compared to CSS where you have no way to nest without writing an entirely new rule set, CSS (Lisp-style) allows you to specify rules for the current selector, and then nest any children in that same rule set. For example, a common use case when styling a navigation bar that uses an unordered list behind the scenes is to get rid of the styling for both the list and the links within the nav bar. In regular CSS, this is two separate rule sets (one for &lt;code>ul&lt;/code> and one for &lt;code>ul li a&lt;/code>), but in our CSS, it’s just one straight line.&lt;/p>
&lt;pre>&lt;code>(ul list-style none (li (a text-decoration none)))
&lt;/code>&lt;/pre>
&lt;p>Allowing the children to be specified as sub-S-expressions is immensely satisfying to me, as I find it better clarifies the relationship between the selectors. Of course, we can also have CSS properties that are more than one word, or contain spaces, the most common being the margin shorthand taking up to four parameters of the border shorthand taking three. For this, I overload the parentheses to create lists. In proper Lisp notation, these would start with a single quote.&lt;/p>
&lt;pre>&lt;code>(body margin (0px 8px 0px 8px) border (1px solid black))
&lt;/code>&lt;/pre>
&lt;p>This is actually not ambiguous to the parser because all rules come in pairs of two: property and value. The nested S-expressions will then be those that don’t follow a property. And what about if you have two S-expressions right next to each other? How do we tell them apart from a rule? Simple: properties cannot contain spaces and thus cannot be listed in parentheses. Therefore, in this case we would have two S-expressions and not a rule.&lt;/p>
&lt;p>If you would like to try this out for yourself, the code is &lt;a href="https://github.com/dustinnewman/cascading-style-s-expressions">open-source&lt;/a> but who wants to go through all the work of pulling and compiling my parser? For that reason, there is also an &lt;a href="https://dustinnewman.net/cascading-style-s-expressions/">online demo&lt;/a> I put together with my first time using WASM. The page is pre-populated with the stylings of that page and you can dynamically edit the CSS to see your Lisp applied. I might end up using this on my own site, the very one you’re reading right now, but time will tell if I have the patience to integrate all of this into Hugo or not.&lt;/p></description></item><item><title>How to use CSS with native dark mode</title><link>https://dustinnewman.net/posts/dark-mode-css/</link><pubDate>Mon, 16 Dec 2019 12:00:00 -0700</pubDate><guid>https://dustinnewman.net/posts/dark-mode-css/</guid><description>&lt;p>With iOS 13, I have been enjoying not just the beautiful new dark mode, but the automatic transition after sunset. It&amp;rsquo;s easier on my eyes and keeps me conscious of using technology too much before bed. That said, the native dark mode is only for native apps. Or is it?&lt;/p>
&lt;p>I thought it would be nice if there were CSS selectors capable of targeting users&amp;rsquo; native preferences on the matter and luckily Apple did not disappoint. In &lt;a href="https://webkit.org/blog/8840/dark-mode-support-in-webkit/">this tutorial post&lt;/a> from the WebKit blog, they introduce the &lt;code>color-scheme&lt;/code> CSS property, which supports both &lt;code>light&lt;/code> and &lt;code>dark&lt;/code> values. I was drawn to this solution over the DIY option (which usually involves some sort of moon icon) because it offers:&lt;/p>
&lt;ol>
&lt;li>Less UI complexity&lt;/li>
&lt;li>More seamless integration with the user&amp;rsquo;s system&lt;/li>
&lt;li>No JavaScript logic required&lt;/li>
&lt;/ol>
&lt;p>All wins in my book, so let&amp;rsquo;s get started! (For this blog, I use SCSS, but I only use the SCSS variables for media queries and the rest of this tutorial uses regular CSS variables.)&lt;/p>
&lt;p>Add this snippet to your top-most styling file (for me, it was my &lt;code>main.scss&lt;/code>):&lt;/p>
&lt;pre>&lt;code>:root {
color-scheme: light dark;
}
&lt;/code>&lt;/pre>
&lt;p>This tells the browser that your site supports both &lt;code>light&lt;/code> and &lt;code>dark&lt;/code> themes. By itself, however, it&amp;rsquo;s not super useful, so let&amp;rsquo;s define some variables that we can switch depending on the theme.&lt;/p>
&lt;pre>&lt;code>:root {
color-scheme: light dark;
--bg-color: #fcfcff;
--text-color: #000000;
}
&lt;/code>&lt;/pre>
&lt;p>Here, we use an off-white as the background color and pure black as the text color. I strongly recommend using functional names rather than descriptive (i.e. &lt;code>bg-color&lt;/code> instead of &lt;code>off-white&lt;/code>) because it allows us to use the same variable declarations with only one media query, rather than using a media query each time we want to use either &lt;code>off-white&lt;/code> or the dark-theme counterpart.&lt;/p>
&lt;p>Now the magic!&lt;/p>
&lt;pre>&lt;code>@media (prefers-color-scheme: dark) {
:root {
--bg-color: #121212;
--text-color: #fcfcfc;
}
}
&lt;/code>&lt;/pre>
&lt;p>With one media query, we re-define our variables so that the &lt;em>text&lt;/em> is now off-white and the background is off-black. To apply this, let&amp;rsquo;s use our main &lt;code>body&lt;/code> element as an example.&lt;/p>
&lt;pre>&lt;code>body {
background-color: var(--bg-color);
color: var(--text-color);
}
&lt;/code>&lt;/pre>
&lt;p>Now our &lt;code>body&lt;/code> will use the &lt;code>bg-color&lt;/code> variable which, if the user prefers dark mode, will be an off-black! Nice! I hope this mini-tutorial was helpful and that you see the advantages of this way over rolling your own moon icon; although I&amp;rsquo;ll admit that might be a bit more fun.&lt;/p></description></item></channel></rss>